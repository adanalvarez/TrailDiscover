{
    "eventName": "InvokeModel",
    "eventSource": "bedrock.amazonaws.com",
    "awsService": "Bedrock",
    "description": "Invokes the specified Amazon Bedrock model to run inference using the prompt and inference parameters provided in the request body.",
    "mitreAttackTactics": [
        "TA0007 - Discovery",
        "TA0040 - Impact"
    ],
    "mitreAttackTechniques": [
        "T1580 - Cloud Infrastructure Discovery",
        "T1496 - Resource Hijacking"
    ],
    "mitreAttackSubTechniques": [],
    "unverifiedMitreAttackTechniques": [
        {
            "technique": "T1020 - Automated Exfiltration",
            "reason": "The InvokeModel API call can be scripted to run repeatedly, allowing for the continuous extraction of data. For example, an attacker could automate requests to the API, each time providing new or varied prompts that extract different pieces of sensitive information"
        },
        {
            "technique": "T1567 - Exfiltration Over Web Service",
            "reason": "An attacker who has access to AWS credentials can set up a process where InvokeModel API calls are made to generate sensitive information in small chunks. Each chunk of data, once generated, can be immediately sent to an S3 bucket or another cloud storage service controlled by the attacker. This method ensures that data is consistently moved out of the compromised environment without raising alarms associated with large data transfers."
        },
        {
            "technique": "T1203 - Exploitation for Client Execution",
            "reason": "Exploiting vulnerabilities in a model's interface could trigger unintended code execution through the InvokeModel API."
        }
    ],
    "usedInWild": true,
    "incidents": [
        {
            "description": "LLMjacking: Stolen Cloud Credentials Used in New AI Attack",
            "link": "https://sysdig.com/blog/llmjacking-stolen-cloud-credentials-used-in-new-ai-attack/"
        },
        {
            "description": "Detecting AI resource-hijacking with Composite Alerts",
            "link": "https://www.lacework.com/blog/detecting-ai-resource-hijacking-with-composite-alerts"
        },
        {
            "description": "New tactics and techniques for proactive threat detection",
            "link": "https://reinforce.awsevents.com/content/dam/reinforce/2024/slides/TDR432_New-tactics-and-techniques-for-proactive-threat-detection.pdf"
        },
        {
            "description": "When AI Gets Hijacked: Exploiting Hosted Models for Dark Roleplaying",
            "link": "https://permiso.io/blog/exploiting-hosted-models"
        }
    ],
    "researchLinks": [],
    "securityImplications": "Attackers might use InvokeModel to check if the credentials have access to the LLMs and they have been enabled and invoke the model for resource hijacking.",
    "alerting": [],
    "simulation": [
        {
            "type": "commandLine",
            "value": "N/A"
        }
    ],
    "permissions": "https://aws.permissions.cloud/iam/bedrock#bedrock-InvokeModel"
}